---
title: "Computational Musicology Portfolio Plan"
author: "Leander van Boven - 12997080"
#break: true
#runtime: shiny
output: 
  flexdashboard::flex_dashboard:
    theme: united
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
#knitr::opts_chunk$set(fig.width = 10, fig.height = 5)

library(ggplot2)
library(plotly)
library(tidyverse)
library(reshape2)
library(factoextra)
library(cluster)
library(spotifyr)
library(shiny)
library(grid)
library(gridExtra)
library(compmus)
load("spotify_env_vars.RData")
load("data/all_songs.RData")
SetEnvVars()

#save(all_songs, file='data/all_songs.RData')
```

Introduction
===

Column
---

### The Idea

For my project I'm going to do research in the genre of hardstyle music.
A lot of people would say that the music within this genre is all alike. However there is a common assumption that each artist distinguishes him- (or her)self with his (or her) unique style and sound. This are most noticable in the tones used in the so called _drop_ and as bass-kick.

I'm going to research whether this assumption can be proved with a (computer)model. In particular a classification model that can classify a song with an artist (assumed that this song is of one of the artists used for training the model). Because if such model can be used we can assume that indeed there is something in the songs that are unique for each artist.
However if such model is not possible, I'm going to research why this is the case, or what is necessary to create such model.

Column
---

### The Corpus

To do this research we obviously need some data to work with.
For this I'm going to use the songs from the top 5 hardstyle artists together with the songs of my 2 most favorite artists.
Together this gives me a corpus of 698 songs where each artist has about 50 songs or more.
This should be enough data to build a decent classification model.

Artist             | Songs on Spotify
------------------ | ----------------
Noisecontrollers   | 199
Headhunterz        | 146
Brennan Heart      | 100
Showtek            | 88
Da Tweekaz         | 62
Sub Zero Project   | 56
D-Block & S-te-Fan | 47

The Data {.storyboard}
===

### __Data Understanding__<br/>In this section I do a quick exploration of the data. I look if there may be already some patterns. {data-commentary-width=600}

```{r First Scatterplots}
dance <- ggplot(all_songs, aes(x=num, y=danceability, col=artist_name)) + geom_point() +
  labs(color='Artist', title='Scatterplot of Danceability') +
  xlab('Track Num') +
  ylab('Danceability') +
  theme(legend.position='left')
energy <- ggplot(all_songs, aes(x=num, y=energy, col=artist_name)) + geom_point() + 
  labs(color='Artist', title='Scatterplot of Energy') +
  xlab('Track Num') +
  ylab('Energy') +
  theme(legend.position='none')
key <- ggplot(all_songs, aes(x=key, fill=artist_name)) + geom_histogram(bins=12) + 
  labs(fill='Artist', title='Histograms of Key') +
  xlab('Key') +
  ylab('Count') +
  facet_grid(~ artist_name) + 
  theme(strip.text=element_text()) +
  theme(legend.position='none')
loud <- ggplot(all_songs, aes(x=num, y=loudness, col=artist_name)) + geom_point() + 
  labs(color='Artist', title='Scatterplot of Loudness') +
  xlab('Track Num') +
  ylab('Loudness') +
  theme(legend.position='none')
mode <- ggplot(all_songs, aes(x=mode, fill=artist_name)) + geom_histogram(bins=2) + 
  labs(fill='Artist', title='Histogram of Mode') +
  xlab('Mode') +
  ylab('Count') +
  facet_grid( ~ artist_name) +
  scale_x_continuous(breaks=c(0,1)) +
  theme(strip.text=element_text()) +
  theme(legend.position='none')
speech <- ggplot(all_songs, aes(x=num, y=speechiness, col=artist_name)) + geom_point() + 
  labs(color='Artist', title='Scatterplot of Speechiness') +
  xlab('Track Num') +
  ylab('Speechiness') +
  theme(legend.position='none')
acoust <- ggplot(all_songs, aes(x=num, y=acousticness, col=artist_name)) + geom_point() + 
  labs(color='Artist', title='Scatterplot of Acousticness') +
  xlab('Track Num') +
  ylab('Acousticness') +
  theme(legend.position='none')
inst <- ggplot(all_songs, aes(x=num, y=instrumentalness, col=artist_name)) + geom_point() + 
  labs(color='Artist', title='Scatterplot of Instrumentalness') +
  xlab('Track Num') +
  ylab('Instrumentalness') +
  theme(legend.position='none')
live <- ggplot(all_songs, aes(x=num, y=liveness, col=artist_name)) + geom_point() + 
  labs(color='Artist', title='Scatterplot of Liveness') +
  xlab('Track Num') +
  ylab('Liveness') +
  theme(legend.position='none')
valence <- ggplot(all_songs, aes(x=num, y=valence, col=artist_name)) + geom_point() + 
  labs(color='Artist', title='Scatterplot of Valence') +
  xlab('Track Num') +
  ylab('Valence') +
  theme(legend.position='none')
tempo <- ggplot(all_songs, aes(x=num, y=tempo, col=artist_name)) + geom_point() + 
  labs(color='Artist', title='Scatterplot of Tempo') +
  xlab('Track Num') +
  ylab('Tempo') +
  theme(legend.position='right')

dance
energy
key
loud
mode
speech
acoust
inst
live
valence
tempo

#ggplotly(dance)
#ggplotly(energy)
#ggplotly(key)
#ggplotly(loud)
#ggplotly(mode)
#ggplotly(speech)
#ggplotly(acoust)
#ggplotly(inst)
#ggplotly(live)
#ggplotly(valence)
#ggplotly(tempo)
```

***

#### Data Understanding

Before trying to build a classifier we first need to do some exploration on and understanding of the data.
In the first place we need to decide which information we are going to use for the classifier.
For example the genre for each artist probably will be similar and thus will not be useful data.
Furthermore we have two possible sets of features we can use to train the classifier with:

* _Track Features_, these features are returned by the `get_track_audio_features()` method of the `spotifyr` package. This method is also used to get the track features in the `get_artist_audio_features()` and `get_album_audio_features()` methods.
  These features are values that say something about the song in a whole, thus we will get 1 feature value per song.
* _Track Analyis_, these features are obtained using the `get_track_audio_analysis()` method fromt the `spotifyr` package.
The analysis features are quite a bit more extensive than the track features, thus will probably contain a lot more information about the song. However this means more data, which will take up more disk space, take longer to obtain from Spotify, make the classifiation training take more time and make the model quite a bit more complex (since we now need to add a time dimension to our model).

Because of the reasons described above I'm first going to focus on creating a model created with the _track features_. 
If I fail to create a good model with these features I'm going to take a look at the _track analysis_ features.

The track features include many features including the following numeric features that may be useful:

* Danceability
* Energy
* Key
* Loudness
* Mode
* Speechiness
* Accousticness
* Instrumentalness
* Liveness
* Valence
* Tempo

This is a lot of data in which some features may be very similar for all songs. It is useless to include this data in the trainingsdata for the classifier since it wouldn't provide good information to distinguish two songs from each other, let alone different artists.

To give a good insight in these features and get a quick overview of which of these may be show some clear differences between the artists, I have combined all songs from all artists into one dataset. I have plotted each of them here (I would have made them interactive however due to limitations of the _plotly_ package they are just images. From these plots you can observe that the _Mode_ features is not useful. Some other features don't seem to show any clear patterns on their own too. That is why I have plotted each feature relative to the other features. If one of these plots already show clusters we probably only need to use these two features to train a classifier with. However I have put these plots in a separate [document](https://github.com/Saltpile123/CM_Portfolio/blob/master/plots/All_features_relative.md) because of the amount (121) and because no plot seems to show any clear clusters.
So although the scatterplots did seem to show some nice patterns, these patterns seem to be very similar among the artists.
This thus means we need to perform Principal Component Analysis.
I will elaborate more on this in the Data Preparation section. If the PCA provides us with good clusters we know that we can quite easily build a classifier, however if the PCA doesn't provide us with any noticable clusters it may be possible that the data still can be clustered, but in higher dimensions. This, however, is quite hard to visualize, thus then I will probably just feed the data to the classifier and hopefully it will be able to draw relations between the features and the artist.

### __Data Preparation__ (Dimensionality Reduction)<br/>Not all data is useful for a classifier, therefore we need to reduce the dimensionality of our dataset. {data-commentary-width=700}

```{r PCA & Clusters, fig.width=10, fig.align='center'}
all_songs_pca <- prcomp(all_songs[,1:11])
plt_pca <- ggplot(all_songs, aes(all_songs_pca$x[,1], all_songs_pca$x[,2], col=all_songs$artist_name)) + 
  geom_point() +
  labs(color='Artist', title='Figure 1: PC2 plotted against PC1') +
  xlab('Frist Principal Component') +
  ylab('Second Principal Component')

pam_feat <- pam(all_songs[1:11], 7)
pam_pca <- pam(all_songs_pca$x, 7)

feat_plt <- fviz_cluster(pam_feat, data=all_songs[1:11], geom='point', main='Figure 2: K-Means on Track Features')
#plotly_build(feat_plt)

pca_plt <- fviz_cluster(pam_pca, data=all_songs_pca$x, geom='point', main='Figure 3: K-Means on Principal Components')
#plotly_build(pca_plt)

plt_pca
#feat_plt
#pca_plt

grid.arrange(feat_plt, pca_plt, nrow=1)
```

***

#### Dimensionality Reduction
Before we can feed the data to the classifier we first need to prepare the data. One part of data preparation is data reduction. This means that we reduce the initial dataset to be only data we are going to use for the classifier.
Since we are going to use the track features we can all discard all data other than these features. I combined this data only into a new data frame and saved that as my new corpus. 
Previously I mentioned removing the _Mode_ feature from our data as well, however since we are now going to perform PCA I will keep this feature for now.
Of course we still need to include the _Artist_ in our reduced dataset since we need to use that data as classes for the model.

#### Principal Component Analysis
As mentioned in the Data Understanding we need to apply Principal Component Analysis on the data since the features on their own or relative to one other feature didn't show any good clusters. Principal component analysis means that we reduce the data to a new dataset where each column is an information rich column that captures as much possible variation from the initial data.
This data may be even better to use than the features on their own since the PCA data will be more dense in information, and will contain only the relevant parts of the features.
First of all I made a PCA of the data, I have plotted the first two against each other (see _Figure 1_).

As you can see, there is still no clear separation between the artists, if any we see that they all seem to be very close in terms of PC1 and PC2.
However since the PCA consists, in our case, of 11 principal components, we might not see all differences between the artists. Thus we need a way to see whether __all__ principal components can be used to desinguish the artists. 

#### K-Means Clustering
We can use clustering for this and k-means clustering in particular. This clustering algorithm first assigns each data point to a random cluster. Then it iteratively calculates the centerpoint of each cluster and assigns each data point to the cluster with the nearest centerpoint, this is done using some distance formula that handles multidimensional data (like Euclidean distance, Manhattan distance).
To compare the standard features with the principal components I have applied k-means clustering on both the features data and principal components data with 7 clusters (we have 7 artists). 

If the data can be effectively clustered we would see 7 separate circles, and in each cirlce there will be only one type of points.
However, as you can see in _Figure 2_ and _Figure 3_ (can be found interactive in the next to stories) most data points are all in one big cluster and the circles of all clusters all cross each other.
We do see that using the Principal Components as data for the clustering algorithm did help to separate one cluster from all others.
But if we examine this cluster we see that 6 out of the 7 artists have songs in this cluster. 

```{r Cluster 1}
unique(all_songs[pam_pca$clustering==1,][,12])
```

### __Figure 2__ (interactive)

```{r}
plotly_build(feat_plt)
```

### __Figure 3__ (interactive)

```{r}
plotly_build(pca_plt)
```

### __Data Preparation__ (Subsetting)<br/>Before we can train a classifier we need training- and testingdata. To reduce data loss I use cross validation.

```{r No Code}

```

#### (Upcoming) Subsetting the data for the classifier
Since we are going to train a classifier we also need to seperate the data into two subsets:

* A trainingset, containing about 80% of the data. This data will be used to train the classifier.
* A testset, containing the remaining 20% of the data. This data will be used to test the classifier.

However, since taking only 20% of the data as validation data, this means we get only 140 songs to validate the classifier. In the best scenario this will mean 20 songs per artist, however since we don't have an equal amount of songs per artist we will most likely not get 20 songs per artist in the validation data.

For this reason I'm going to perform cross validation on the data with 5 batches. This means that I'm going to shuffle the data and then divide the data into 5 parts (thus each part is 20% of the data). After that I will repeatedly take one part as testset and the other parts as trainingset.
This way all data will be once testdata and 4 times trainingsdata, resulting in a model that has seen more data and thus is less overfitted on that data, hopefully giving a model that can classify novel songs better.

Testing on Novel Data
===

Column {.tabset}
---

### __Novel Data__<br/>I gather the songs that are released after I made my initial corpus, to use as novel test data. {data-commentary-width=750}

#### Novel Data

As mentioned earlier, I want to test my models and techniques on novel data; songs that have been released by one of the artists of my corpus after I made my initial corpus.
A few songs have been released already.
I have put some of them below:

Artist             | Track Name
------------------ | ----------------
D-Block & S-te-Fan | [Lake Of Fire](https://open.spotify.com/track/0ri3xd7X6jvJQTz9T6IE7l?si=orVc36F-Q-mjiiHyejSp9g)
D-Block & S-te-Fan | [Feel It!](https://open.spotify.com/track/01xdiGuSuIf5qZulm1uqkY?si=Papt4K18RSqLVy-3tXJnVg)
D-Block & S-te-Fan | [Fallen Souls](https://open.spotify.com/track/7dOk9Bck00Zv0eeq5m6GoN?si=bVXEr2b0RraA8akTgbVj-g)
Brennan Heart | [Born & Raised](https://open.spotify.com/track/1UFkRFGEDkTafQF55tiQvY?si=IPAmFsVvTDuKx-mGG7vjwQ)
Da Tweekaz | [Power Of Perception](https://open.spotify.com/track/3QFVJxsqDK6ThOX5CLSBOM?si=bc_UEpYJRdmUF6Eu9w7lzQ)
Da Tweekaz | [Knockout](https://open.spotify.com/track/5LSCfqeq1u1GxKogSQFEZn?si=l2ljGiQxRKK4lCmmviuxxw)
Da Tweekaz | [We Made It](https://open.spotify.com/track/6yhvInrBQThvos9CNDJV8H?si=61bAoSpYTViDr6nDZV00mg)

### __Song Similarity using Chroma Features__<br/> I compare some songs by comparing their chromagrams.

One way to maybe see to whether two songs belong to the same artist would be by using chromagrams. From the list of novel songs we see that both _D-Block & S-te-Fan_ and _Da Tweekaz_ have released the most songs in the past weeks. So I choose to use 2 songs from _D-Block & S-te-Fan_ and one song from _Da Tweekaz_:

* _Lake Of Fire_ as comparison song.
* _Fallen Souls_ as base-line song for _D-Block & S-te-Fan_.
* _Knockout_ as base-line song for _Da Tweekaz_.

Then I created a _Dynamic Time Warping_ plot between the comparison and one base-line song to look if we can see some similarities between both songs from _D-Block & S-te-Fan_ and big difference between de comparison song and the base-line song of _Da Tweekaz_.

The plots can be found below:

```{r Chromagrams}
#load('data/novel_samples.RData')

lake_of_fire <- 
    get_tidy_audio_analysis('0ri3xd7X6jvJQTz9T6IE7l') %>% 
    select(segments) %>% unnest(segments) %>% 
    select(start, duration, pitches)
fallen_souls <- 
  get_tidy_audio_analysis('7dOk9Bck00Zv0eeq5m6GoN') %>% 
  select(segments) %>% unnest(segments) %>% 
  select(start, duration, pitches)

db_st_plt <- compmus_long_distance(
    lake_of_fire %>% mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')),
    fallen_souls %>% mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')),
    feature = pitches,
    method = 'euclidean') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    scale_fill_continuous(type = 'viridis', guide = 'none') +
    labs(x = 'Lake Of Fire (D-Block & S-te-Fan)', y = 'Fallen Souls (D-Block & S-te-Fan)') +
    theme_minimal()

knockout <- 
  get_tidy_audio_analysis('5LSCfqeq1u1GxKogSQFEZn') %>% 
  select(segments) %>% unnest(segments) %>% 
  select(start, duration, pitches)

dbst_dt_plt <- compmus_long_distance(
    lake_of_fire %>% mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')),
    knockout %>% mutate(pitches = map(pitches, compmus_normalise, 'chebyshev')),
    feature = pitches,
    method = 'euclidean') %>% 
    ggplot(
        aes(
            x = xstart + xduration / 2, 
            width = xduration,
            y = ystart + yduration / 2,
            height = yduration,
            fill = d)) + 
    geom_tile() +
    scale_fill_continuous(type = 'viridis', guide = 'none') +
    labs(x = 'Lake Of Fire (D-Block & S-te-Fan)', y = 'Knockout (Da Tweekaz)') +
    theme_minimal()

db_st_plt
dbst_dt_plt
#ggplotly(dbst_dt_plt)
```

In my opinion the DTW between the two _D-Block & S-te-Fan_ songs looks a lot more symmetrical, which would indicate that both songs both follow a similar pattern. This while in DTW between the _D-Block & S-te-Fan_ song and  the _Da Tweekaz_ song both songs are very destinguishable, which would indicate that they are very dissimilar from each other.

We now need to find a way to convert this dissimilarity into a way to link an artist with a song.

Upcoming {.storyboard}
===

> All items below are a planning and description of things I'm going to do in the upcoming weeks, these are subject to change as result of feedback and/or new things learned during the course.

### __Modeling__ <br/>This is where the actual models are created, and we finally see what they are capable of!

As mentioned before I'm going to train multiple models, trained on different subsets of the initial dataset:

* A model trained on all features mentioned in the **Data Understanding** without the _Mode_ feature.
* A model trained on (a subset of) the Principal Components.
* (Optionally) A model trained on (a subset of) the `get_track_audio_analysis()` features, if necessary.

Depending on the results of these models I may deduce what is causing them to perform a certain way, and build new models that may perform better.

### __Evaluation__ <br/>Here I evaluate the performance of each model and explore the reason of their performance.

For each model we can assess it's performance by the amount of songs it classified correctly.
We can use an extensive confusion matrix to compare the amount of correctly classified songs to the amount of incorrect classifications to see which songs are related to each other.
After all, if the songs for a certain artist that were incorrectly classified almost always were classified with a certain other artist, those artists must be very similar to each other in terms of the features used for the model.

Optimally we want to create a model that uses only the features returned by the `get_track_audio_features()` method, without diving into the sounds and tune of a song (the data returned by the `get_track_audio_analysis()` method). I'm going to compare the different models with each other and find which model performed best and why.
If two artists turn out to be very similar in each model, I may look for the most similar tracks and subjectively compare them by for example listening to them to see whether those tracks are indeed similar to the ear as well.

For the future I hope that some of the artists used to build the model will release new songs, thus allowing me to test the model on novel data to see whether the model is indeed as good as it proved to be on the testdata.


IDEE:
lake of fire vergelijken met een random song van elke andere artiest
maar ff kijken hoe en wat
